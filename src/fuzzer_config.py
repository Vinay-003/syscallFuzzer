#!/usr/bin/env python3
"""
fuzzer_config.py - Coverage-Guided Syscall Fuzzer with KCOV

FINAL VERSION with metric tracking for "Slide 7"
Features:
- KCOV-based code coverage tracking
- PC-based coverage tracking (stubbed, requires executor.c modification)
- Proper buffer and FD allocation for sequences
- Resource pool management for valid file descriptors
- Intelligent crash detection (distinguishes errors from crashes)
- Corpus management for interesting inputs
- Comprehensive logging and statistics for 3-minute test runs
"""

import subprocess
import time
import os
import random
import re
import json
import signal
import sys
from datetime import datetime
from pathlib import Path
import shutil

# Import fuzzing logic
# We need this for the final stats calculation
try:
    from fuzzer_brain import SYSCALL_SPECS, SYSCALL_SEQUENCES, TYPE_GENERATORS
except ImportError:
    print("[!] Warning: fuzzer_brain.py not found.")
    print("[!] Stats for 'Syscalls tested %' will not be available.")
    SYSCALL_SPECS = {} # Dummy variable
    SYSCALL_SEQUENCES = {}
    TYPE_GENERATORS = {}

# ============================================================================
# CONFIGURATION
# ============================================================================

class Config:
    """Central configuration for the fuzzer"""
    # VM Settings
    VM_DISK_IMAGE = "../alpine.qcow2"
    VM_RAM = "1G"
    HOST_SSH_PORT = "10022"
    VM_USER = "root"
    VM_PASSWORD = "123"
    
    # Paths
    EXECUTOR_SOURCE = "executor.c"
    EXECUTOR_VM_PATH = "/root/executor"
    CRASHES_DIR = "crashes"
    CORPUS_DIR = "corpus"
    
    # Timeouts (seconds)
    VM_BOOT_TIMEOUT = 60
    SSH_READY_TIMEOUT = 30
    SSH_COMMAND_TIMEOUT = 20
    VM_SHUTDOWN_TIMEOUT = 30
    
    # Fuzzing behavior
    SEQUENCE_PROBABILITY = 0.3  # 30% chance to run sequence vs single syscall
    ITERATION_DELAY = 0.05  # seconds between iterations
    
    # === NEW: METRIC TEST CONFIG ===
    SESSION_DURATION_MINUTES = 3 # Duration for the "Slide 7" test run
    STATS_UPDATE_INTERVAL = 5 # Seconds, for calculating peak throughput
    
    # === NEW: KCOV A/B TEST SWITCH ===
    # Set this to False to compile/run *without* KCOV
    # to measure overhead
    ENABLE_KCOV = True
    
    # Set this to your mmap'd buffer size in executor.c
    KCOV_BUFFER_SIZE_MB = 64 
    
    # Coverage thresholds
    MIN_INTERESTING_COVERAGE_PCS = 1 # Min *new* PCs to be interesting
    
    # Resource allocation sizes
    BUFFER_SIZE = 65536  # 64KB buffer for various operations
    PIPE_FD_ARRAY_SIZE = 8  # Space for pipe FD pairs
    
    @staticmethod
    def get_qemu_command():
        """Generate QEMU command with optimal settings"""
        return [
            "qemu-system-x86_64",
            "-m", Config.VM_RAM,
            "-hda", Config.VM_DISK_IMAGE,
            "-nographic",
            "-netdev", f"user,id=net0,hostfwd=tcp::{Config.HOST_SSH_PORT}-:22",
            "-device", "e1000,netdev=net0",
            "-enable-kvm"  # Use KVM for better performance
        ]


# ============================================================================
# CRASH LOGGING
# ============================================================================

class CrashLogger:
    """Handles crash detection and artifact collection"""
    
    def __init__(self, crashes_dir):
        self.crashes_dir = Path(crashes_dir)
        self.crashes_dir.mkdir(exist_ok=True)
        self.crash_count = 0
    
    def log_crash(self, reproducer_commands, crash_context=None, console_log_path=None):
        """
        Save comprehensive crash information
        
        Args:
            reproducer_commands: Command(s) that caused the crash
            crash_context: Additional context (dict with metadata)
            console_log_path: Path to the VM's console log file
        
        Returns:
            Path to crash directory
        """
        self.crash_count += 1
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        crash_dir = self.crashes_dir / f"crash_{timestamp}"
        crash_dir.mkdir(exist_ok=True)
        
        print("\n" + "!" * 70)
        print(f"{'⚠️  POTENTIAL CRASH DETECTED  ⚠️':^70}")
        print(f"{'Saving to: ' + str(crash_dir):^70}")
        print("!" * 70)
        
        # Save reproducer script
        reproducer_file = crash_dir / "reproducer.sh"
        with open(reproducer_file, "w") as f:
            f.write("#!/bin/bash\n")
            f.write("# Crash reproducer generated by syscall fuzzer\n")
            f.write(f"# Timestamp: {timestamp}\n")
            f.write(f"# Crash #{self.crash_count}\n\n")
            
            if isinstance(reproducer_commands, (list, tuple)):
                for cmd in reproducer_commands:
                    f.write(f"{cmd}\n")
            else:
                f.write(f"{reproducer_commands}\n")
        
        reproducer_file.chmod(0o755)
        
        # Save metadata
        metadata = {
            "timestamp": timestamp,
            "crash_number": self.crash_count,
            "commands": reproducer_commands if isinstance(reproducer_commands, list) else [reproducer_commands],
            "crash_context": crash_context or {}
        }
        
        with open(crash_dir / "metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
        
        # Save summary
        with open(crash_dir / "crash_summary.txt", "w") as f:
            f.write("=== Syscall Fuzzer Crash Report ===\n")
            f.write(f"Timestamp: {timestamp}\n")
            f.write(f"Crash Number: #{self.crash_count}\n")
            f.write(f"Crash Directory: {crash_dir}\n\n")
            
            f.write("--- Reproducer Commands ---\n")
            if isinstance(reproducer_commands, (list, tuple)):
                for i, cmd in enumerate(reproducer_commands, 1):
                    f.write(f"{i}. {cmd}\n")
            else:
                f.write(f"{reproducer_commands}\n")
            
            if crash_context:
                f.write("\n--- Additional Context ---\n")
                for key, value in crash_context.items():
                    f.write(f"{key}: {value}\n")

        # Copy console log for kernel panic analysis
        if console_log_path and console_log_path.exists():
            try:
                shutil.copy(console_log_path, crash_dir / "console.log")
                print(f"[+] Saved VM console log")
            except Exception as e:
                print(f"[!] Failed to copy console log: {e}")
        
        print(f"[+] Crash artifacts saved to: {crash_dir}")
        return crash_dir


# ============================================================================
# SSH OPERATIONS
# ============================================================================

class SSHRunner:
    """Handles SSH communication with the VM"""
    
    def __init__(self, host="localhost", port=None, user=None, password=None):
        self.host = host
        self.port = port or Config.HOST_SSH_PORT
        self.user = user or Config.VM_USER
        self.password = password or Config.VM_PASSWORD
        self.ssh_base_opts = [
            "-o", "StrictHostKeyChecking=no",
            "-o", "UserKnownHostsFile=/dev/null",
            "-o", "ConnectTimeout=10",
            "-o", "LogLevel=ERROR"
        ]
    
    def check_ssh_ready(self):
        """Check if SSH port is accepting connections"""
        try:
            result = subprocess.run(
                ["nc", "-z", self.host, self.port],
                timeout=2,
                capture_output=True
            )
            return result.returncode == 0
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
            return False
    
    def run_command(self, command, suppress_errors=False, timeout=None):
        """
        Execute command in VM via SSH
        
        Returns:
            CompletedProcess on success, None on failure/timeout
        """
        timeout = timeout or Config.SSH_COMMAND_TIMEOUT
        
        ssh_cmd = [
            "sshpass", "-p", self.password,
            "ssh", f"{self.user}@{self.host}",
            "-p", self.port
        ] + self.ssh_base_opts + [command]
        
        try:
            if not suppress_errors:
                print(f"[VM] {command}")
            
            result = subprocess.run(
                ssh_cmd,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            # Return result regardless of exit code - let caller decide
            if not suppress_errors and result.stdout.strip():
                print(f"[OUTPUT] {result.stdout.strip()}")
            
            return result
                
        except subprocess.TimeoutExpired:
            if not suppress_errors:
                print(f"[!] Command timed out after {timeout}s")
            return None
        except Exception as e:
            if not suppress_errors:
                print(f"[!] SSH error: {e}")
            return None
    
    def transfer_file(self, local_path, remote_path):
        """Copy file from host to VM"""
        print(f"[*] Transfer: {local_path} → VM:{remote_path}")
        
        scp_cmd = [
            "sshpass", "-p", self.password,
            "scp", "-P", self.port
        ] + self.ssh_base_opts + [
            local_path,
            f"{self.user}@{self.host}:{remote_path}"
        ]
        
        try:
            subprocess.run(scp_cmd, check=True, capture_output=True, timeout=30)
            print("[+] Transfer successful")
            return True
        except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:
            print(f"[!] Transfer failed: {e}")
            return False


# ============================================================================
# VM MANAGEMENT
# ============================================================================

class VMManager:
    """Manages QEMU VM lifecycle"""
    
    def __init__(self, ssh_runner):
        self.ssh_runner = ssh_runner
        self.vm_process = None
        self.console_log_path = Path("console.log")
        self.console_log_file = None
    
    def start_vm(self):
        """Start VM and wait for SSH readiness"""
        print("\n[*] Starting VM...")
        
        try:
            # Open console log for capturing kernel messages
            self.console_log_file = open(self.console_log_path, "wb")
            self.vm_process = subprocess.Popen(
                Config.get_qemu_command(),
                stdout=self.console_log_file,
                stderr=self.console_log_file
            )
        except Exception as e:
            print(f"[!] Failed to start VM: {e}")
            if self.console_log_file:
                self.console_log_file.close()
            return False
        
        print(f"[+] VM started (PID: {self.vm_process.pid})")
        
        # Wait for SSH port to open
        print("[*] Waiting for SSH port...")
        start_time = time.time()
        
        while time.time() - start_time < Config.VM_BOOT_TIMEOUT:
            if self.ssh_runner.check_ssh_ready():
                print("[+] SSH port open")
                time.sleep(5)  # Let SSH service fully initialize
                
                # Verify SSH is actually responsive
                if self._verify_ssh_responsive():
                    print("[+] VM ready for fuzzing")
                    return True
                
            time.sleep(1)
        
        print("[!] VM boot timeout")
        self.terminate_vm()
        return False
    
    def _verify_ssh_responsive(self):
        """Verify SSH is actually responsive"""
        print("[*] Verifying SSH responsiveness...")
        
        start_time = time.time()
        while time.time() - start_time < Config.SSH_READY_TIMEOUT:
            result = self.ssh_runner.run_command(
                "echo SSH_OK",
                suppress_errors=True
            )
            
            if result and result.returncode == 0 and "SSH_OK" in (result.stdout or ""):
                return True
            
            time.sleep(2)
        
        return False
    
    def shutdown_vm(self, force=False):
        """Gracefully shutdown VM"""
        if not self.vm_process:
            if self.console_log_file:
                self.console_log_file.close()
            return
        
        if force:
            print("[*] Force terminating VM...")
            self.vm_process.terminate()
        else:
            print("[*] Shutting down VM gracefully...")
            self.ssh_runner.run_command("poweroff", suppress_errors=True, timeout=10)
            
            try:
                self.vm_process.wait(timeout=Config.VM_SHUTDOWN_TIMEOUT)
            except subprocess.TimeoutExpired:
                print("[!] Graceful shutdown timeout, forcing...")
                self.vm_process.terminate()
        
        if self.console_log_file:
            self.console_log_file.close()
        self.vm_process = None
        print("[+] VM stopped")
    
    def terminate_vm(self):
        """Immediately terminate VM"""
        self.shutdown_vm(force=True)
    
    def is_running(self):
        """Check if VM is still running"""
        if not self.vm_process:
            return False
        return self.vm_process.poll() is None
    
    def check_for_kernel_panic(self):
        """
        Check console log for kernel panic indicators
        
        Returns:
            True if panic detected, False otherwise
        """
        if not self.console_log_path.exists():
            return False
        
        try:
            with open(self.console_log_path, 'rb') as f:
                content = f.read().decode('utf-8', errors='ignore')
                
            # Common kernel panic indicators
            panic_indicators = [
                "Kernel panic", "Kernel Panic", "kernel panic - not syncing",
                "not syncing:", "Attempted to kill init!", "Fatal exception",
                "general protection fault", "unable to handle kernel",
                "page fault in", "Segmentation fault", "Oops:", "BUG:",
                "BUG: kernel NULL pointer dereference", "kernel BUG at",
                "KASAN:", "KMSAN:", "KCSAN:", "UBSAN:", "lockdep:",
                "RCU stall", "watchdog: BUG", "watchdog: BUG: soft lockup",
                "watchdog: BUG: hard LOCKUP", "soft lockup - CPU", "hard LOCKUP",
                "hung task", "task hung", "WARNING: CPU:", "stack segment:",
                "stack overflow in", "bad stack state", "EXT4-fs error", "XFS (",
                "BTRFS critical", "FAT-fs (", "I/O error", "end_request: I/O error",
                "Buffer I/O error", "Machine check", "Machine check events logged",
                "Internal error:", "Illegal instruction", "Bad page state",
                "bad paging request", "bad kernel paging request",
                "Unable to handle kernel paging request", "Instruction fetch fault",
                "Double fault", "Triple fault", "Fatal signal",
                "kernel NULL pointer dereference", "BUG: unable to handle page fault",
                "Modules linked in:",
            ]
            
            return any(indicator.lower() in content.lower() for indicator in panic_indicators)
        except Exception:
            return False


# ============================================================================
# FUZZING ENGINE WITH PROPER RESOURCE ALLOCATION
# ============================================================================

class FuzzingEngine:
    """Core fuzzing logic with KCOV coverage tracking and resource management"""
    
    # Regex to parse executor output with coverage AND pc_data
    RE_EXECUTOR_OUTPUT = re.compile(
        r"syscall\((\d+)\)\s*=\s*(-?\d+)"
        r"(?:.*?coverage=(\d+))?"  # Optional coverage count
        r"(?:.*?pc_data=([a-fA-F0-9x,]+))?" # <-- NEW: Capture PC data
    )

    
    def __init__(self, ssh_runner, crash_logger, vm_manager):
        self.ssh_runner = ssh_runner
        self.crash_logger = crash_logger
        self.vm_manager = vm_manager
        self.iteration = 0
        
        # --- NEW: PC-based Coverage Tracking ---
        # This set will store all unique PC addresses (e.g., '0xffffffff810000a0')
        self.total_coverage_pcs = set()
        self.syscalls_hit = set() # For 'Syscalls tested %' metric
        
        # Corpus management
        self.corpus_dir = Path(Config.CORPUS_DIR)
        self.corpus_dir.mkdir(exist_ok=True)
        
        # Resource pool for valid file descriptors and buffers
        self.resource_pool = {
            "fd": [],  # Valid file descriptors
            "valid_buffer": None,  # Allocated buffer address
            "pipe_fds": None,  # Pipe FD array address
            "timer_id_buffer": None,  # Timer ID buffer
        }
        
        # --- NEW: Statistics for Slide 7 ---
        self.stats = {
            "total_syscalls": 0,
            "total_sequences": 0,
            "errors": 0,  # Expected errors (ENOENT, EINVAL, etc.)
            "crashes": 0,  # Actual crashes/panics
            "new_coverage_inputs": 0, # Inputs that added new coverage
            "iterations": 0,
            "peak_throughput": 0.0,
            "initial_coverage_pcs": 0,
            "final_coverage_pcs": 0,
            "first_crash_time": -1.0, # Relative to session start
            "first_crash_iter": -1,
            "session_start_time": 0.0,
            "session_end_time": 0.0,
        }
        
        # For peak throughput calculation
        self.last_stat_check_time = time.time()
        self.last_stat_check_syscalls = 0
        
        print("[*] Fuzzing engine initialized")
        print(f"[*] Corpus directory: {self.corpus_dir}")
    
    def allocate_resources(self):
        """
        Allocate persistent resources needed for sequences
        Returns True on success, False on failure
        """
        print("\n[*] Allocating fuzzing resources...")
        
        # Allocate main buffer using mmap
        print(f"[*] Allocating {Config.BUFFER_SIZE} byte buffer...")
        status, buffer_addr, new_pcs = self.execute_syscall(
            "mmap",
            ["0", str(Config.BUFFER_SIZE), "3", "34", "-1", "0"],  # PROT_READ|WRITE, MAP_PRIVATE|ANON
            check_crash=False
        )
        if new_pcs: # Track coverage from setup
             self.total_coverage_pcs.update(new_pcs)
        
        if status == 'success' and buffer_addr and buffer_addr > 0:
            self.resource_pool["valid_buffer"] = buffer_addr
            print(f"[+] Buffer allocated at 0x{buffer_addr:x}")
            
            # Write some data to the buffer
            self.ssh_runner.run_command(
                f"echo 'user.test' | dd of=/proc/self/mem bs=1 count=10 seek={buffer_addr} 2>/dev/null || true",
                suppress_errors=True
            )
        else:
            print("[!] Failed to allocate main buffer")
            return False
        
        # Allocate pipe FD array buffer
        print(f"[*] Allocating pipe FD array...")
        status, pipe_fds_addr, new_pcs = self.execute_syscall(
            "mmap",
            ["0", str(Config.PIPE_FD_ARRAY_SIZE), "3", "34", "-1", "0"],
            check_crash=False
        )
        if new_pcs: self.total_coverage_pcs.update(new_pcs)
        
        if status == 'success' and pipe_fds_addr and pipe_fds_addr > 0:
            self.resource_pool["pipe_fds"] = pipe_fds_addr
            print(f"[+] Pipe FD array allocated at 0x{pipe_fds_addr:x}")
        else:
            print("[!] Warning: Failed to allocate pipe FD array")
        
        # Allocate timer ID buffer
        status, timer_buf_addr, new_pcs = self.execute_syscall(
            "mmap",
            ["0", "16", "3", "34", "-1", "0"],
            check_crash=False
        )
        if new_pcs: self.total_coverage_pcs.update(new_pcs)

        if status == 'success' and timer_buf_addr and timer_buf_addr > 0:
            self.resource_pool["timer_id_buffer"] = timer_buf_addr
            print(f"[+] Timer ID buffer allocated at 0x{timer_buf_addr:x}")
        
        # Open some file descriptors to populate the FD pool
        print("[*] Creating initial file descriptors...")
        for path in ["/tmp/fuzzfile", "/tmp/testfile"]:
            status, fd, new_pcs = self.execute_syscall(
                "open",
                [path, "66", "420"],  # O_RDWR|O_CREAT, 0644
                check_crash=False
            )
            if new_pcs: self.total_coverage_pcs.update(new_pcs)
            if status == 'success' and fd and fd >= 0:
                self.resource_pool["fd"].append(fd)
                print(f"[+] Created FD {fd} for {path}")
        
        # Open /dev/null and /dev/zero
        for path in ["/dev/null", "/dev/zero"]:
            status, fd, new_pcs = self.execute_syscall(
                "open",
                [path, "2", "0"],  # O_RDWR
                check_crash=False
            )
            if new_pcs: self.total_coverage_pcs.update(new_pcs)
            if status == 'success' and fd and fd >= 0:
                self.resource_pool["fd"].append(fd)
                print(f"[+] Opened FD {fd} for {path}")
        
        print(f"[+] Resource allocation complete!")
        print(f"    - Buffer: 0x{self.resource_pool['valid_buffer']:x}")
        print(f"    - Pipe FDs: 0x{self.resource_pool.get('pipe_fds', 0):x}")
        print(f"    - Valid FDs: {len(self.resource_pool['fd'])} descriptors")
        
        return True
    
    def log_initial_coverage(self):
        """
        Called once after allocate_resources() to set the
        baseline coverage for the 30-min run.
        """
        initial_pcs = len(self.total_coverage_pcs)
        self.stats["initial_coverage_pcs"] = initial_pcs
        print(f"[+] Initial coverage after setup: {initial_pcs} PCs")
    
    def parse_executor_output(self, stdout):
        """
        Extract syscall return value and coverage from executor output
        
        Returns:
            (return_value: int, coverage_count: int, new_pcs: set)
        """
        if not stdout:
            return None, 0, set()
        
        match = self.RE_EXECUTOR_OUTPUT.search(stdout)
        if match:
            try:
                ret_val = int(match.group(2))
                coverage_count = int(match.group(3)) if match.group(3) else 0
                
                # --- THIS IS THE NEW PARSING LOGIC ---
                new_pcs = set()
                pc_data_str = match.group(4) # Get the captured pc_data string
                
                if pc_data_str:
                    # Split the comma-separated string and add to the set
                    new_pcs = set(pc.strip() for pc in pc_data_str.split(',') if pc.strip())
                # --- END OF NEW LOGIC ---
                    
                return ret_val, coverage_count, new_pcs
            except (ValueError, IndexError):
                return None, 0, set()
        return None, 0, set()
    
    def is_interesting_input(self, new_pcs_from_input):
        """
        Determine if an input provided new, unseen PCs
        
        Args:
            new_pcs_from_input: A set of PCs from the last execution
            
        Returns:
            (is_interesting: bool, newly_discovered_pcs: set)
        """
        if not new_pcs_from_input:
            return False, set()
        
        # Check for PCs that are not in our global set
        newly_discovered_pcs = new_pcs_from_input - self.total_coverage_pcs
        
        if len(newly_discovered_pcs) >= Config.MIN_INTERESTING_COVERAGE_PCS:
            return True, newly_discovered_pcs
        
        return False, set()
    
    def save_to_corpus(self, command, new_pc_count):
        """Save interesting input to corpus"""
        filename = self.corpus_dir / f"cov_{new_pc_count}_iter_{self.iteration}"
        
        try:
            with open(filename, "w") as f:
                f.write(command)
            print(f"[+] Saved to corpus: {filename.name}")
            return True
        except Exception as e:
            print(f"[!] Failed to save to corpus: {e}")
            return False
    
    def update_resource_pool(self, syscall_name, ret_val):
        """
        Update resource pool with valid resources created by syscalls
        
        Args:
            syscall_name: Name of the syscall executed
            ret_val: Return value from the syscall
        """
        # File descriptor-creating syscalls
        fd_creating_syscalls = [
            "open", "openat", "socket", "epoll_create", "epoll_create1",
            "eventfd", "eventfd2", "timerfd_create", "signalfd", "signalfd4",
            "memfd_create", "userfaultfd", "pipe", "pipe2", "dup", "dup2", "dup3",
            "inotify_init", "inotify_init1", "fanotify_init", "perf_event_open",
            "io_uring_setup", "pidfd_open"
        ]
        
        if syscall_name in fd_creating_syscalls and ret_val is not None and ret_val >= 0:
            # Valid FD created
            if ret_val not in self.resource_pool["fd"]:
                self.resource_pool["fd"].append(ret_val)
                print(f"[+] Added FD {ret_val} to resource pool (total: {len(self.resource_pool['fd'])})")
                
                # Keep pool size reasonable
                if len(self.resource_pool["fd"]) > 30:
                    removed = self.resource_pool["fd"].pop(0)
                    print(f"[*] Removed old FD {removed} from pool")
    
    def resolve_argument(self, arg_spec, env):
        """
        Resolve argument specification to concrete value
        
        Args:
            arg_spec: Argument specification (string, dict, or literal)
            env: Environment dict with stored results from sequence execution
        
        Returns:
            String representation of the argument value
        """
        # Handle literal value from sequence (e.g., {'literal': 123})
        if isinstance(arg_spec, dict) and "literal" in arg_spec:
            return str(arg_spec["literal"])
        
        # Handle environment variable reference (e.g., {'value': 'fd1'})
        if isinstance(arg_spec, dict) and "value" in arg_spec:
            var_name = arg_spec["value"]
            
            # Check for special resource pool variables
            if var_name == "valid_buffer":
                return str(self.resource_pool.get("valid_buffer", 0))
            elif var_name == "pipe_fds":
                return str(self.resource_pool.get("pipe_fds", 0))
            elif var_name == "timer_id_buffer":
                return str(self.resource_pool.get("timer_id_buffer", 0))
            # Handle pipe FD extraction (pipe_read_fd, pipe_write_fd)
            elif var_name == "pipe_read_fd":
                # Read FD is at pipe_fds[0]
                return str(env.get("pipe_read_fd", env.get("pipe_fd", 3)))
            elif var_name == "pipe_write_fd":
                # Write FD is at pipe_fds[1]
                return str(env.get("pipe_write_fd", env.get("pipe_fd", 4)))
            # Handle socketpair FD extraction
            elif var_name == "socket_fd1":
                return str(env.get("socket_fd1", env.get("socket_fd", 3)))
            elif var_name == "socket_fd2":
                return str(env.get("socket_fd2", env.get("socket_fd", 4)))
            
            return str(env.get(var_name, 0))
        
        # Handle type generator
        if isinstance(arg_spec, str):
            # Special handling for valid_fd - use resource pool
            if arg_spec == "valid_fd":
                if self.resource_pool["fd"]:
                    chosen_fd = random.choice(self.resource_pool["fd"])
                    return str(chosen_fd)
                else:
                    # Pool empty, return a common FD
                    return "3"
            
            # Special handling for valid_buffer
            if arg_spec == "valid_buffer":
                return str(self.resource_pool.get("valid_buffer", 0))
            
            # Special handling for pipe_fds
            if arg_spec == "pipe_fds":
                return str(self.resource_pool.get("pipe_fds", 0))
            
            # Check if it's an environment variable
            if arg_spec in env:
                return str(env[arg_spec])
            
            # Check if it's a generator type
            generator = TYPE_GENERATORS.get(arg_spec)
            if generator:
                try:
                    return str(generator())
                except Exception:
                    return "0"
            
            # Return as-is (literal string or path)
            return arg_spec
        
        # Return literal value
        return str(arg_spec)
    
    def generate_random_syscall(self):
        """Generate a random syscall with arguments"""
        syscall_name = random.choice(list(SYSCALL_SPECS.keys()))
        arg_types = SYSCALL_SPECS[syscall_name]
        args = [self.resolve_argument(arg_type, {}) for arg_type in arg_types]
        return syscall_name, args
    
    def execute_syscall(self, syscall_name, args, check_crash=True):
        """
        Execute a single syscall
        
        Args:
            syscall_name: Name of the syscall
            args: List of argument values
            check_crash: Whether to check for crashes
        
        Returns:
            (status: str, return_value: int, new_pcs: set)
        """
        command = f"{Config.EXECUTOR_VM_PATH} {syscall_name} {' '.join(args)}"
        result = self.ssh_runner.run_command(command, suppress_errors=True)
        
        # Check for timeout (potential hang/crash)
        if result is None:
            if check_crash and self.vm_manager.check_for_kernel_panic():
                return 'crash', None, set()
            return 'timeout', None, set()
        
        # Parse output - this now returns all 3 values
        ret_val, coverage_count, new_pcs = self.parse_executor_output(result.stdout)
        
        # Update resource pool if syscall succeeded
        if ret_val is not None and ret_val >= 0:
            self.update_resource_pool(syscall_name, ret_val)
        
        # Determine status
        if result.returncode != 0:
            if "Error:" in result.stderr or result.returncode == 124:
                return 'timeout', ret_val, new_pcs
            elif check_crash and self.vm_manager.check_for_kernel_panic():
                return 'crash', ret_val, new_pcs
            else:
                return 'error', ret_val, new_pcs
        
        # Return all three values
        return 'success', ret_val, new_pcs

    def execute_sequence(self, sequence_name, steps):
        """
        Execute a sequence of syscalls with environment tracking
        
        Returns:
            (status: str, commands_executed: list, all_new_pcs: set)
        """
        print(f"\n[*] Executing sequence: {sequence_name}")
        
        env = {}  # Environment for storing intermediate results
        commands = []
        all_new_pcs = set() # Aggregate PCs from all steps
        
        # Special handling for pipe/socketpair sequences
        # Pre-populate env with expected FD values
        if "pipe" in sequence_name or "splice" in sequence_name:
            env["pipe_read_fd"] = 3  # Typical first available FD
            env["pipe_write_fd"] = 4
        if "socketpair" in sequence_name:
            env["socket_fd1"] = 3
            env["socket_fd2"] = 4
        
        for step_num, step in enumerate(steps, 1):
            syscall_name = step.get("action") or step.get("name")
            arg_specs = step.get("args", [])
            
            # Resolve arguments using environment
            args = [self.resolve_argument(spec, env) for spec in arg_specs]
            
            # Build command string
            command = f"{Config.EXECUTOR_VM_PATH} {syscall_name} {' '.join(args)}"
            commands.append(command)
            
            print(f"  [{step_num}/{len(steps)}] {syscall_name}({', '.join(args)})")
            
            # Execute syscall
            status, ret_val, new_pcs = self.execute_syscall(syscall_name, args, check_crash=True)
            all_new_pcs.update(new_pcs) # Aggregate PCs
            
            # Handle result
            if status == 'crash':
                print(f"  [!] Crash detected at step {step_num}")
                return 'crash', commands, all_new_pcs
            elif status == 'timeout':
                print(f"  [!] Timeout at step {step_num}")
                # Continue sequence even on timeout (might be interesting)
            
            # Store return value in environment if requested
            if step.get("result"):
                result_var = step["result"]
                env[result_var] = ret_val if ret_val is not None else -1
                print(f"    → {result_var} = {env[result_var]}")
                
                # Special handling for pipe() - extract both FDs
                if syscall_name in ["pipe", "pipe2"] and ret_val == 0:
                    # pipe() returns 0 on success, FDs are written to the array
                    # We'll simulate this by assigning sequential FDs
                    env["pipe_read_fd"] = 3  # Assume first available FD
                    env["pipe_write_fd"] = 4
                    print(f"    → pipe_read_fd = {env['pipe_read_fd']}")
                    print(f"    → pipe_write_fd = {env['pipe_write_fd']}")
                
                ## Special handling for socketpair() - extract both FDs
                if syscall_name == "socketpair" and ret_val == 0:
                    # socketpair() returns 0 on success, FDs are written to the array
                    env["socket_fd1"] = 3  # Assume first available FD
                    env["socket_fd2"] = 4
                    print(f"    → socket_fd1 = {env['socket_fd1']}")
                    print(f"    → socket_fd2 = {env['socket_fd2']}")
        
        print(f"[+] Sequence '{sequence_name}' completed")
        return 'success', commands, all_new_pcs
    
    #
    # --- THIS IS THE CORRECTED FUNCTION ---
    #
    def run_iteration(self):
        """
        Run a single fuzzing iteration
        
        Returns:
            bool: True if should continue, False if crash detected
        """
        self.iteration += 1
        self.stats["iterations"] += 1
        
        print(f"\n{'=' * 70}")
        print(f"Iteration #{self.iteration}")
        print('=' * 70)
        
        # Decide: single syscall or sequence?
        run_sequence = (
            random.random() < Config.SEQUENCE_PROBABILITY and 
            SYSCALL_SEQUENCES
        )
        
        if run_sequence:
            # Execute sequence
            sequence_name, steps = random.choice(list(SYSCALL_SEQUENCES.items()))
            status, commands, all_new_pcs = self.execute_sequence(sequence_name, steps)
            
            self.stats["total_sequences"] += 1
            self.stats["total_syscalls"] += len(steps)
            
            # Log syscalls tested
            for step in steps:
                self.syscalls_hit.add(step.get("action") or step.get("name"))

            # --- CORRECTED LOGIC ---
            # 1. Check for interesting coverage FIRST
            is_new, new_pcs_found = self.is_interesting_input(all_new_pcs)
                
            if status == 'crash':
                # Real crash detected
                self.stats["crashes"] += 1
                if self.stats["first_crash_time"] == -1:
                    self.stats["first_crash_time"] = time.time() - self.stats["session_start_time"]
                    self.stats["first_crash_iter"] = self.iteration
                
                crash_dir = self.crash_logger.log_crash(
                    commands,
                    crash_context={
                        "type": "sequence",
                        "sequence_name": sequence_name,
                        "iteration": self.iteration,
                        "stats": self.stats.copy()
                    },
                    console_log_path=self.vm_manager.console_log_path
                )
                return False  # Stop this session
            elif status in ('timeout', 'error'):
                self.stats["errors"] += 1
            
            # 2. If it's new, NOW update global set and save to corpus
            if is_new:
                print(f"\n{'*' * 25} NEW COVERAGE! (Sequence) {'*' * 25}")
                self.total_coverage_pcs.update(new_pcs_found) # Update global set
                self.stats["new_coverage_inputs"] += 1
                self.save_to_corpus("\n".join(commands), len(new_pcs_found))
            # --- END CORRECTION ---
        
        else:
            # Execute single syscall
            syscall_name, args = self.generate_random_syscall()
            self.syscalls_hit.add(syscall_name) # Log syscall tested
            command = f"{Config.EXECUTOR_VM_PATH} {syscall_name} {' '.join(args)}"
            
            print(f"[*] Testing: {syscall_name}({', '.join(args)})")
            
            status, ret_val, new_pcs = self.execute_syscall(syscall_name, args, check_crash=True)
            
            self.stats["total_syscalls"] += 1

            # --- CORRECTED LOGIC ---
            # 1. Check for interesting coverage FIRST
            is_new, new_pcs_found = self.is_interesting_input(new_pcs)
            
            # Handle crash
            if status == 'crash':
                self.stats["crashes"] += 1
                if self.stats["first_crash_time"] == -1:
                    self.stats["first_crash_time"] = time.time() - self.stats["session_start_time"]
                    self.stats["first_crash_iter"] = self.iteration

                crash_dir = self.crash_logger.log_crash(
                    command,
                    crash_context={
                        "type": "single",
                        "syscall": syscall_name,
                        "args": args,
                        "iteration": self.iteration,
                        "stats": self.stats.copy()
                    },
                    console_log_path=self.vm_manager.console_log_path
                )
                return False  # Stop this session
            
            # Handle error (expected syscall failures)
            elif status == 'error':
                self.stats["errors"] += 1
                print(f"[*] Syscall returned error (expected)")
            
            # 2. If it's new, NOW update global set and save to corpus
            if is_new:
                print(f"\n{'*' * 25} NEW COVERAGE! (Single) {'*' * 25}")
                self.total_coverage_pcs.update(new_pcs_found) # Update global set
                self.stats["new_coverage_inputs"] += 1
                self.save_to_corpus(command, len(new_pcs_found))
            # --- END CORRECTION ---
        
        # --- NEW: Calculate peak throughput ---
        current_time = time.time()
        time_delta = current_time - self.last_stat_check_time
        
        if time_delta > Config.STATS_UPDATE_INTERVAL:
            syscall_delta = self.stats["total_syscalls"] - self.last_stat_check_syscalls
            
            if time_delta > 0:
                current_throughput = syscall_delta / time_delta
                if current_throughput > self.stats["peak_throughput"]:
                    self.stats["peak_throughput"] = current_throughput
            
            self.last_stat_check_time = current_time
            self.last_stat_check_syscalls = self.stats["total_syscalls"]
        
        return True  # Continue fuzzing
    
    def run_session(self):
        """
        Run a complete fuzzing session for the configured duration
        
        Returns:
            str: 'interrupted', 'crash', or 'time_limit'
        """
        print("\n" + "#" * 70)
        print(f"{'FUZZING SESSION START':^70}")
        print(f"Duration: {Config.SESSION_DURATION_MINUTES} minutes")
        print("#" * 70)
        
        # Allocate resources before fuzzing
        if not self.allocate_resources():
            print("[!] Failed to allocate resources")
            return 'error'
        
        # Set initial coverage *after* allocation
        self.log_initial_coverage()
        
        self.stats["session_start_time"] = time.time()
        test_duration_sec = Config.SESSION_DURATION_MINUTES * 60
        
        try:
            # Main loop is now time-based
            while (time.time() - self.stats["session_start_time"]) < test_duration_sec:
                should_continue = self.run_iteration()
                
                if not should_continue:
                    # Crash detected
                    self.stats["session_end_time"] = time.time()
                    return 'crash'
                
                time.sleep(Config.ITERATION_DELAY)
            
            # Reached time limit
            self.stats["session_end_time"] = time.time()
            print(f"\n[+] Session time limit ({Config.SESSION_DURATION_MINUTES} min) reached.")
            return 'time_limit'
                
        except KeyboardInterrupt:
            print("\n[!] Session interrupted by user")
            self.stats["session_end_time"] = time.time()
            return 'interrupted'
        
    def _get_dir_size_mb(self, path):
        """Helper to get directory size for stats"""
        try:
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(path):
                for f in filenames:
                    fp = os.path.join(dirpath, f)
                    if not os.path.islink(fp):
                        total_size += os.path.getsize(fp)
            return total_size / (1024 * 1024) # Return size in MB
        except Exception:
            return 0.0
    
    def print_stats(self):
        """Print comprehensive fuzzing statistics (for Slide 7)"""
        print("\n" + "=" * 70)
        print(f"FINAL RESULTS – {Config.SESSION_DURATION_MINUTES} MINUTE RUN")
        print("=" * 70)
        
        start_time = self.stats["session_start_time"]
        end_time = self.stats["session_end_time"]
        
        if start_time == 0.0 or end_time <= start_time:
            print("[!] No session data recorded, cannot print stats.")
            return
            
        run_duration_sec = end_time - start_time
        run_duration_min = run_duration_sec / 60.0
        
        # --- Performance Metrics ---
        print("\n--- Performance Metrics ---")
        total_syscalls = self.stats["total_syscalls"]
        if total_syscalls > 0:
            ms_per_syscall = (run_duration_sec * 1000.0) / total_syscalls
            sustained_throughput = total_syscalls / run_duration_sec
            
            print(f"Total syscalls:       {total_syscalls}")
            print(f"Sustained Throughput: {sustained_throughput:.2f} syscalls/sec")
            print(f"Peak Throughput:      {self.stats['peak_throughput']:.2f} syscalls/sec")
            print(f"Execution Time:       {ms_per_syscall:.2f} ms per syscall")
        else:
            print("Total syscalls:       0")

        print(f"KCOV Overhead:        [Run A/B test with ENABLE_KCOV=False]")
        
        # --- Resource Usage ---
        print("\n--- Resource Usage ---")
        corpus_size_mb = self._get_dir_size_mb(self.corpus_dir)
        crash_size_mb = self._get_dir_size_mb(self.crash_logger.crashes_dir)
        corpus_inputs = len(list(self.corpus_dir.glob('*')))
        
        print(f"CPU (Host):           [Measure externally with 'top' on qemu proc]")
        print(f"CPU (Guest):          [Measure externally with 'ssh' + 'top']")
        print(f"Memory (Host):        [Measure externally with 'top' on qemu proc]")
        print(f"Memory (Guest):       [Measure externally with 'ssh' + 'top']")
        print(f"KCOV Buffer:          {Config.KCOV_BUFFER_SIZE_MB} MB (from Config)")
        print(f"Storage (Corpus):     {corpus_size_mb:.2f} MB ({corpus_inputs} inputs)")
        print(f"Storage (Crashes):    {crash_size_mb:.2f} MB")
        
        # --- Coverage Statistics ---
        print("\n--- Coverage Statistics ---")
        initial_pcs = self.stats["initial_coverage_pcs"]
        final_pcs = len(self.total_coverage_pcs)
        self.stats["final_coverage_pcs"] = final_pcs # Save for later
        
        total_new_pcs = final_pcs - initial_pcs
        growth_rate_pcs_per_hour = 0
        if run_duration_min > 0:
            growth_rate_pcs_per_hour = (total_new_pcs) / (run_duration_min / 60.0)
        
        print(f"Initial PCs:          {initial_pcs}")
        print(f"Final PCs:            {final_pcs} (Total new: {total_new_pcs})")
        print(f"Growth Rate:          {growth_rate_pcs_per_hour:.2f} PCs/hour")

        if SYSCALL_SPECS:
            total_possible_syscalls = len(SYSCALL_SPECS)
            hit_syscalls = len(self.syscalls_hit)
            hit_percent = (hit_syscalls / total_possible_syscalls) * 100.0
            print(f"Syscalls tested:      {hit_syscalls} of {total_possible_syscalls} ({hit_percent:.1f}%)")
        else:
            print(f"Syscalls tested:      {len(self.syscalls_hit)} (fuzzer_brain not found)")

        if self.stats["first_crash_time"] != -1:
            crash_time_min = self.stats["first_crash_time"] / 60.0
            print(f"First crash:          {crash_time_min:.2f} minutes (iteration {self.stats['first_crash_iter']})")
        else:
            print(f"First crash:          None found")
            
        # --- Reproducibility ---
        print("\n--- Reproducibility ---")
        print("Reproducibility:    [Run external crash reproduction script 20x]")
        print("Consistent Coverage:  [Run this 30-min test 5x and compare Final PCs]")
        print("=" * 70)


# ============================================================================
# SETUP AND INITIALIZATION
# ============================================================================

def setup_executor(ssh_runner):
    """Transfer and compile executor in VM, setup KCOV"""
    print("\n" + "=" * 70)
    print("EXECUTOR SETUP")
    print("=" * 70)
    
    # Create test files in VM
    print("[*] Creating test files...")
    ssh_runner.run_command("touch /tmp/fuzzfile /tmp/testfile")
    ssh_runner.run_command("chmod 666 /tmp/fuzzfile /tmp/testfile")
    
    # --- NEW: Conditional KCOV Setup ---
    if Config.ENABLE_KCOV:
        # Mount debugfs for KCOV
        print("[*] Mounting debugfs for KCOV...")
        result = ssh_runner.run_command(
            "mountpoint -q /sys/kernel/debug || mount -t debugfs none /sys/kernel/debug"
        )
        if not result or result.returncode != 0:
            print("[!] Warning: Could not mount debugfs (KCOV may not work)")
        
        # Verify KCOV is available
        print("[*] Verifying KCOV availability...")
        result = ssh_runner.run_command(
            "test -e /sys/kernel/debug/kcov && echo 'KCOV_OK'",
            suppress_errors=True
        )
        if not result or "KCOV_OK" not in result.stdout:
            print("[!] CRITICAL: KCOV not available - coverage tracking will not work")
            print("[!] Make sure you compiled the kernel with KCOV support")
            return False
        else:
            print("[+] KCOV is available")
    else:
        print("[*] KCOV is DISABLED (for A/B overhead testing)")
    
    # Transfer source
    if not ssh_runner.transfer_file(Config.EXECUTOR_SOURCE, "/root/executor.c"):
        print("[!] Failed to transfer executor source")
        return False
    
    # Install build tools if needed
    print("[*] Installing build tools...")
    result = ssh_runner.run_command(
        "which gcc || apk add --no-cache build-base",
        timeout=120
    )
    if not result:
        print("[!] Failed to install build tools")
        return False
    
    # Compile executor
    print("[*] Compiling executor...")
    
    # --- NEW: Conditional Compilation ---
    compile_cmd = f"gcc -O2 -Wall /root/executor.c -o {Config.EXECUTOR_VM_PATH}"
    if Config.ENABLE_KCOV:
        print("[+] Compiling with KCOV logic enabled (default).")
        # If your C code needs a flag:
        # compile_cmd += " -DENABLE_KCOV" 
    else:
        print("[!] Compiling with KCOV logic DISABLED.")
        # You MUST use this flag in your executor.c to #ifdef out
        # all KCOV-related calls (ioctl, mmap, etc.)
        compile_cmd += " -DDISABLE_KCOV" 
    
    result = ssh_runner.run_command(compile_cmd, timeout=60)
    
    if not result or result.returncode != 0:
        print("[!] Compilation failed")
        if result:
            print(f"[!] Error: {result.stderr}")
        return False
    
    # Verify executable exists
    result = ssh_runner.run_command(
        f"test -x {Config.EXECUTOR_VM_PATH} && echo 'EXECUTOR_OK'",
        suppress_errors=True
    )
    if not result or "EXECUTOR_OK" not in result.stdout:
        print("[!] Executor not found after compilation")
        return False
    
    print("[+] Executor ready")
    return True


# ============================================================================
# MAIN ORCHESTRATION
# ============================================================================

def main():
    """Main fuzzer orchestration"""
    print("\n" + "#" * 70)
    print(f"{'COVERAGE-GUIDED SYSCALL FUZZER':^70}")
    print(f"{'WITH PROPER RESOURCE ALLOCATION':^70}")
    print("#" * 70)
    
    # Initialize random seed
    seed = int(time.time() * 1000) & 0xFFFFFFFF
    random.seed(seed)
    print(f"[*] Random seed: {seed}")
    
    # Initialize components
    ssh_runner = SSHRunner()
    vm_manager = VMManager(ssh_runner)
    crash_logger = CrashLogger(Config.CRASHES_DIR)
    
    # Initial setup - start VM once to compile executor
    print("\n[*] Performing initial setup...")
    if not vm_manager.start_vm():
        print("[!] Initial VM startup failed")
        return 1
    
    if not setup_executor(ssh_runner):
        print("[!] Executor setup failed")
        vm_manager.terminate_vm()
        return 1
    
    # Shutdown VM for clean fuzzing sessions
    print("[*] Setup complete, shutting down for clean sessions...")
    vm_manager.shutdown_vm()
    time.sleep(3)
    
    # Main fuzzing loop
    print("\n" + "#" * 70)
    print(f"{'READY TO FUZZ':^70}")
    print("#" * 70)
    
    # Create fuzzing engine (persistent across sessions)
    fuzzing_engine = FuzzingEngine(ssh_runner, crash_logger, vm_manager)
    session_count = 0
    
    try:
        while True:
            session_count += 1
            print(f"\n{'#' * 70}")
            print(f"{'FUZZING SESSION #' + str(session_count):^70}")
            print(f"{'#' * 70}")
            
            # Start fresh VM for this session
            if not vm_manager.start_vm():
                print("[!] Failed to start VM")
                break
            
            # Run fuzzing session
            result = fuzzing_engine.run_session()
            
            # Print statistics after each session
            fuzzing_engine.print_stats()
            
            # Handle session result
            if result == 'interrupted':
                print("[*] User requested shutdown")
                vm_manager.shutdown_vm()
                break
            elif result == 'crash':
                print("[*] Crash detected - restarting VM for next session")
                vm_manager.terminate_vm()
                time.sleep(3)
            elif result == 'time_limit':
                print("[*] Session complete - restarting VM for fresh session")
                vm_manager.shutdown_vm()
                time.sleep(3)
            elif result == 'error':
                print("[!] Resource allocation error - restarting VM")
                vm_manager.terminate_vm()
                time.sleep(3)
    
    except KeyboardInterrupt:
        print("\n\n[!] Fuzzer interrupted by user")
    except Exception as e:
        print(f"\n[!] Unexpected error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\n[*] Cleaning up...")
        vm_manager.terminate_vm()
        # Ensure stats are printed at the very end
        fuzzing_engine.print_stats()
        print("[+] Fuzzer shutdown complete")
        print(f"\n[*] Total sessions run: {session_count}")
        print(f"[*] Check '{Config.CRASHES_DIR}' for crash reports")
        print(f"[*] Check '{Config.CORPUS_DIR}' for interesting inputs")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

